<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yunshuang (Sheyla) Li</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/head_robo.png" type="image/png">
    <!--<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  </head>
  <style>
      .black-link {
      color: black; /* Set the desired color for the link */
    }
      .myself {
        color: black;
        text-decoration: underline;
      }
  </style>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yunshuang (Sheyla) Li
                </p>
                <p>I am a Robotics master student at GRASP Lab, University of Pennsyvania, where I work at the intersections of robotics, computer vision, and machine learning.
                </p>
                <p>
                  Currently, I'm a member of <a href="https://www.seas.upenn.edu/~dineshj/pal/index.html">PAL research group</a>, advised by <a href="https://www.seas.upenn.edu/~dineshj/">Prof. Dinesh Jayaraman</a>. Previoulsy, I worked with <a href="https://www.cse.cuhk.edu.hk/~qdou/index.html">Prof. Qi Dou</a> at Chinese University of Hongkong (CUHK) for an internship. I received my Honorable B.S. degree from <a href="http://ckc.zju.edu.cn/ckcen/">Chu Kochen Honors College</a>, Zhejiang University in 2022.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sheylali@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Yunshuang_Li_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/Yunshuang-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=fpx2AWYAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/YunshuangL">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Yunshuang_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yunshuang_headshot.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in making robot learning more efficient and robust by studying how the notion of "ATTENTION" can be useful
                  for robots during observation, representation, and policy learning. My career goal is to make robot learning as easy and robust as possible.
                </p>

                <p>
                  (* indicates equal contribution, ‚Ä† indicates equal advising)
                </p>
              </td>
            </tr>
          </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/bakedsdf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                <img src='images/im2contact.png' width="160">
              </div>
              <script type="text/javascript">
                function im2contact_start(){
                  document.getElementById('zipnerf_image').style.opacity = "1";
                }

                function im2contact_stop() {
                  document.getElementById('zipnerf_image').style.opacity = "0";
                }
                im2contact_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=h8halpbqB-&referrer=%5Bthe%20profile%20of%20Dinesh%20Jayaraman%5D(%2Fprofile%3Fid%3D~Dinesh_Jayaraman2)">
                <span class="papertitle">Vision-Based Contact Localization Without Touch or Force Sensing</span>
              </a>
              <br>
              Leon Kim,
              <a class="myself"><strong>Yunshuang Li</strong></a>, 
              <a href="https://www.grasp.upenn.edu/people/michael-posa/">Michael Posa</a>,
              <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2023 &nbsp 
              <br>
              <a href="https://sites.google.com/view/im2contact/home">arXiv</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">project page</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">video</a>
              <p></p>
              <p>
                We propose a challenging vision-based extrinsic contact localization task: with only a single RGB-D camera view of a robot workspace, identify when and where an object held by the robot contacts the rest of the environment. Our final approach im2contact demonstrates the promise of versatile general-purpose contact perception from vision alone, performing well for localizing various contact types (point, line, or planar; sticking, sliding, or rolling; single or multiple), and even under occlusions in its camera view.
              </p>
            </td>
          </tr>

            <tr onmouseout="MICCAI_stop()" onmouseover="MICCAI_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='MICCAI_image'>
                    <img src='images/challenge_after.png' width="160"></div>
                  <img src='images/challenge_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function MICCAI_start() {
                    document.getElementById('MICCAI_image').style.opacity = "1";
                  }

                  function MICCAI_stop() {
                    document.getElementById('MICCAI_image').style.opacity = "0";
                  }
                  MICCAI_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169260723002262">
                  <span class="papertitle">PEg TRAnsfer Workflow recognition challenge report: Do multimodal data improve recognition?</span>
                </a>
                <br>
                Arnaud Huaulm√©, Kanako Harada, (et al., including <a class="myself"><strong>Yunshuang Li</strong></a>, 
                <a href="https://www.researchgate.net/profile/Yonghao-Long-2" class="black-link">Yonghao Long</a>, 
                <a href="https://www.cse.cuhk.edu.hk/~qdou/" class="black-link">Qi Dou</a>)
                <br>
                <em>Computer Methods and Programs in Biomedicine, 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2202.05821">arXiv</a>
                <p></p>
                <p>
                  This is the report paper on Workflow Recognition Challenge in MICCAI 2021. I lead the MedAIR team and rank the first over all the 5 rank method in one sub-challenge on multi-modal (videos and kinematics) workflow recognition of robotic surgery videos.
              </td>
            </tr>		


            <tr onmouseout="Collaborate_stop()" onmouseover="Collaborate_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='Collaborate_image'>
                    <img src='images/collaborative_1.png' width="160"></div>
                  <img src='images/collaborative_2.png' width="160">
                </div>
                <script type="text/javascript">
                  function Collaborate_start() {
                    document.getElementById('Collaborate_image').style.opacity = "1";
                  }

                  function Collaborate_stop() {
                    document.getElementById('Collaborate_image').style.opacity = "0";
                  }
                  Collaborate_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517621">
                  <span class="papertitle">Collaborative Recognition of Feasible Region with Aerial and Ground Robots through DPCN</span>
                </a>
                <br>
                <a class="myself"><strong>Yunshuang Li</strong></a>,
                Zheyuan Huang, 
                <a href="https://www.researchgate.net/profile/Zexi-Chen-3" class="black-link">Zexi Chen</a>, 
                <a href="https://ywang-zju.github.io/" class="black-link">Yue Wang</a>, 
                <a href="https://www.researchgate.net/profile/Rong-Xiong" class="black-link">Rong Xiong</a>
                <br>
                <em>IEEE International Conference on Real-time Computing and Robotics (RCAR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2103.00947v2">arXiv</a>
                <p></p>
                <p>
                  We present a collaborative system with aerial and ground robots to gain precise recognition of feasible region. Taking the aerial robots' advantages of having large scale variance of view points of the same route which the ground robots is on, the collaboration work provides global information of road segmentation for the ground robot, thus enabling it to obtain feasible region and adjust its pose ahead of time.
                </p>
              </td>
            </tr>		

            <tr onmouseout="SNN_stop()" onmouseover="SNN_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='SNN_image'>
                    <img src='images/SNN_after_large.png' width="160"></div>
                  <img src='images/SNN_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function SNN_start() {
                    document.getElementById('SNN_image').style.opacity = "1";
                  }

                  function SNN_stop() {
                    document.getElementById('SNN_image').style.opacity = "0";
                  }
                  SNN_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-90525-5_79">
                  <span class="papertitle">Control of Pneumatic Artificial Muscles with SNN-based Cerebellar-like Model</span>
                </a>
                <br>
                <a href="https://cuhkleggedrobotlab.github.io/people/hongbo/" class="black-link">Hongbo Zhang</a>*,
                <a class="myself"><strong>Yunshuang Li</strong>*</a>,
                Yipin Guo*,
                Xinyi Chen,
                <a href="https://ieeexplore.ieee.org/author/38264350700" class="black-link">Qinyuan Ren</a>
                <br>
                <em>International Conference on Social Robotics (ICSR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2109.10750#:~:text=Inspired%20by%20Cerebellum%27s%20vital%20functions%20in%20control%20of,controlling%20a%201-DOF%20robot%20arm%20driven%20by%20PAMs.">arXiv</a>
                /
                <a href="data/poster_SNN.pdf">poster</a>
                <p></p>
                <p>
                  Inspired by Cerebellum's vital functions in control of human's physical movement, we propose a neural network model of Cerebellum based on spiking neuron networks (SNNs). We apply the model as a feed-forward controller in controlling a 1-DOF robot arm driven by PAMs. 
                </p>
              </td>
            </tr>	

            

      
      <!--Image -> Video no yellow bg
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>-->

          </tbody></table>
          
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Service</h2>
                <p>
                  <b>CIS 5200</b> Machine Learning: TA in Fall 24 at Penn.<br>
                  <b><a href="https://github.com/MEAM520">MEAM 5200 Introduction to Robotics</a></b>: TA in Spring 23 at Penn.<br>
                  <b><a herf="https://fife.cis.upenn.edu">Fife-Penn Python Club</a></b>: Instructor in Spring 23 at G.W. Carver High School.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Adwards</h2>
                <p>
                  <b>CoRL 2023 Travel Grant</b> 2023<br>
                  <b>Summer Internship Award</b> issued by GAPSA at Penn, 2023<br>
                  <b>Chiang Chen Overseas Graduate Fellowship</b> (10 students each year in mainland China, 50k$), 2022<br>
                  <b>Outstanding Graduate</b> of Zhejiang Province issued by Department of Education of Zhejiang Province, 2022<br>
                  <b>National Scholarship</b> issued by the Ministry of Education of PRC, 2021<br>
                  <b>First Prize Scholarship</b> issued by Zhejiang University, 2018-2021
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Inspired by the template <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
